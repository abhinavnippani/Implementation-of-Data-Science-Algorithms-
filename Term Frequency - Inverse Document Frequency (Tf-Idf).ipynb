{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/prash/Downloads/ML ALGORITHMS/'\n",
    "\n",
    "title = \"stories_dataset\"\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Files and Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset is:  467\n"
     ]
    }
   ],
   "source": [
    "# Getting the names of all the folders and storing it\n",
    "folders = [x[0] for x in os.walk(path + 'DATASETS/' + title + '/')]\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "\n",
    "# Form the Dataset\n",
    "\n",
    "dataset = []\n",
    "c = False\n",
    "for i in folders:\n",
    "    file = open(i+\"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "    if c == False:\n",
    "        file_name = file_name[2:]\n",
    "        c = True\n",
    "    \n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))\n",
    "        \n",
    "N = len (dataset)      \n",
    "print(\"Length of the dataset is: \",N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing the Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Documents PreProcessed!!\n",
      "100 Documents PreProcessed!!\n",
      "150 Documents PreProcessed!!\n",
      "200 Documents PreProcessed!!\n",
      "250 Documents PreProcessed!!\n",
      "300 Documents PreProcessed!!\n",
      "350 Documents PreProcessed!!\n",
      "400 Documents PreProcessed!!\n",
      "450 Documents PreProcessed!!\n"
     ]
    }
   ],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    # remove comma seperately\n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    # needed again as we need to stem the words\n",
    "    data = stemming(data) \n",
    "    # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_punctuation(data) \n",
    "    # needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    data = remove_stop_words(data)\n",
    "    return data\n",
    "\n",
    "def print_doc(id):\n",
    "    print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text)\n",
    "    \n",
    "    \n",
    "\n",
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "count = 0\n",
    "for i in dataset[:N]:\n",
    "    \n",
    "    # Get the Text from the file\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    \n",
    "    # PreProcess the text\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    # PreProcess the title\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))\n",
    "    \n",
    "    count += 1\n",
    "    if(count%50 ==0):\n",
    "        print(str(count) + \" Documents PreProcessed!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Vocab Size is:  32350\n",
      "\n",
      "Tf-Idf Value of the word 'go' in Text is:  0.0002906893990853149\n",
      "\n",
      "Tf-Idf Value of the word 'go' in Title is:  0.0002906893990853149\n",
      "\n",
      "Final Length of Tf-Idf Values Are: 344378\n"
     ]
    }
   ],
   "source": [
    "# Get the Vocabulary\n",
    "\n",
    "\n",
    "DF = {}\n",
    "# Get the indexes where each word is present\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "# Calculate the DF score\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "    \n",
    "total_vocab_size = len(DF)\n",
    "print(\"The Total Vocab Size is: \",total_vocab_size)\n",
    "\n",
    "# List of words in Vocabulary\n",
    "total_vocab = [x for x in DF]\n",
    "\n",
    "\n",
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "# Tf-Idf Values for Text\n",
    "\n",
    "tf_idf_text = {}\n",
    "doc = 0\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    # Vocab for combined Text and Title\n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    # Length of combined Text and Title\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        # Calculating Term Frequency\n",
    "        tf = counter[token]/words_count\n",
    "        # Calculating Inverse Document Frequency\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        # Calculating Tf-Idf Values\n",
    "        tf_idf_text[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1\n",
    "\n",
    "print(\"\\nTf-Idf Value of the word 'go' in Text is: \",tf_idf_text[(0,\"go\")])\n",
    "    \n",
    "# Tf-Idf Values for Title\n",
    "    \n",
    "tf_idf_title = {}\n",
    "doc = 0\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_text[i])\n",
    "    words_count = len(tokens + processed_text[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        #numerator is added 1 to avoid negative values\n",
    "        idf = np.log((N+1)/(df+1)) \n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1\n",
    "    \n",
    "print(\"\\nTf-Idf Value of the word 'go' in Title is: \",tf_idf_title[(0,\"go\")])\n",
    "\n",
    "\n",
    "# Combining Both Tf-Idf Values into one\n",
    "\n",
    "tf_idf = copy.deepcopy(tf_idf_text)\n",
    "\n",
    "for i in tf_idf:\n",
    "    tf_idf[i] *= alpha\n",
    "    \n",
    "# Refer to Reference Link for more understanding on this step    \n",
    "for i in tf_idf_title:\n",
    "    tf_idf[i] = tf_idf_title[i]\n",
    "    \n",
    "print(\"\\nFinal Length of Tf-Idf Values Are:\",len(tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum.She stood next a slatted oak bench, canisters still clutched, surveying.\n",
      "\n",
      " ['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
      "\n",
      "The top 10 Documents with the Highest Similarity are:\n",
      "\n",
      " [166, 200, 352, 433, 211, 350, 175, 187, 188, 294]\n"
     ]
    }
   ],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\\n\",tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "    for key in tf_idf:\n",
    "        # If the word exists in tokens\n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    # Sort the documents in the order of highest similarity\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Print the First K documents\n",
    "    l = []\n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(\"\\nThe top \" + str(k) + \" Documents with the Highest Similarity are:\\n\\n\",l)\n",
    "    \n",
    "\n",
    "matching_score(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum.\"\n",
    "                + \"She stood next a slatted oak bench, canisters still clutched, surveying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorising Tf-Idf Values...\n",
      "Cosine Similarity\n",
      "\n",
      "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum.She stood next a slatted oak bench, canisters still clutched, surveying\n",
      "\n",
      " ['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
      "\n",
      "The top 10 Documents with the Highest Similarity are:\n",
      "\n",
      " [200 166 433 175 169 402 211  87 151 369]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q\n",
    "\n",
    "\n",
    "def cosine_similarity(k, query):\n",
    "    \n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"Cosine Similarity\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\\n\",tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    # Finding Tf-Idf of Tokens and Vectorising it\n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "    \n",
    "    # Sort the documents in the order of highest similarity\n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]    \n",
    "    print(\"\\nThe top \" + str(k) + \" Documents with the Highest Similarity are:\\n\\n\",out)\n",
    "\n",
    "\n",
    "# Vectorising Tf-Idf\n",
    "print(\"Vectorising Tf-Idf Values...\")\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "Q = cosine_similarity(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum.\"\n",
    "                     + \"She stood next a slatted oak bench, canisters still clutched, surveying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just an example of Scikit Learn Implementation. The dataset to be passed has to be preprocessed first\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(pd.DataFrame(dataset)[1])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089 <br>\n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
